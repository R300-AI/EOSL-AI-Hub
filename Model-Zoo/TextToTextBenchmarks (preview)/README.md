## AMD Ryzen AI 300 Series

基於 AMD 官方建議，本報告採用 [**Ollama**](https://ollama.com/) 推論框架及[**LLM-Benchmark**](https://pypi.org/project/llm-benchmark/)在本地執行大型語言模型的測試，Ollama對AMD Ryzen AI APU已提供完善的支援，能在 Windows 與 Linux 環境下直接利用 iGPU 做加速，兼顧效能、效率與本地資料隱私。

```bash
llm_benchmark run --custombenchmark=profile_quant.yml
```

> 請注意，執行過程需透過BIOS或Adrenalin Editor開啟`VGM（VRAM iGPU Memory）`。 VGM是 AMD 專為 Ryzen AI 設計的記憶體最佳化技術，能讓推論過程更高效率地載入與處理權重及資料，減少 CPU 與 GPU 間的傳輸瓶頸與延遲，提升整體的推論速度與穩定性。

## Ryzen AI 9 HX

### 量化等級及策略評估

您可以透過表格中的 **CPU使用率、iGPU使用率、TTFT（Time-to-first-Token）**與**生成速度**觀察VGM 在不同量化設定下的效能差異與運算資源分配情形。其中，量化等級以 `qX` 表示，X 為位元數(數值越低壓縮率越高，但模型精度可能會下降)；命名中的`0`、`1`、`K_Small`、`K_Medium`、`K_Large` 則代表同位元下的不同量化變體，允許使用者在速度、資源與精度之間取不同平衡。

  | Model                         |  CPU (%) | iGPU (%) |  TTFT (ms) |  Speed (token/s)  |
  |-------------------------------|----------|----------|---------------|------------|
  | qwen2.5:0.5b                  |    34    |    7     |        33.3   | 101.48     |
  | qwen2.5:0.5b-base-q2_K        |    33    |   10     |        20.6   | 103.79     |
  | qwen2.5:0.5b-base-q3_K_S      |    37    |   12     |        34.0   |  98.25     |
  | qwen2.5:0.5b-base-q3_K_M      |    45    |   11     |        27.9   | 107.18     |  
  | qwen2.5:0.5b-base-q3_K_L      |    38    |   12     |        35.6   |  99.68     |
  | qwen2.5:0.5b-base-q4_0        |    32    |   15     |        23.8   | 102.80     |
  | qwen2.5:0.5b-base-q4_1        |    33    |   10     |        38.0   | 102.73     |
  | qwen2.5:0.5b-base-q4_K_S      |    30    |    7     |        36.6   | 100.43     |
  | qwen2.5:0.5b-base-q4_K_M      |    38    |   13     |        24.7   | 101.02     |
  | qwen2.5:0.5b-base-q5_0        |    36    |   17     |        31.5   |  98.38     |
  | qwen2.5:0.5b-base-q5_1        |    35    |   11     |        27.5   |  94.09     |
  | qwen2.5:0.5b-base-q5_K_S      |    37    |   10     |        18.7   |  99.19     |
  | qwen2.5:0.5b-base-q8_0        |    36    |   12     |        14.7   |  86.69     |

---

### 架構相容性測試

本節彙整了參數量`8B`參數以下可部署於本地端執行的多種主流大型語言模型，並比較其在 AMD Ryzen AI APU 上的實際效能表現，您可以透過以此表格觀察不同架構的模型在推論時的資源分配與速度差異。建議讀者可以進一步搭配前述的量化策略評估，針對應用情境適用的模型版本進一步尋找合適的模型量化，以達到最佳的效能、資源與精度的平衡。
 
  | Model             |  CPU (%) | iGPU (%) |  TTFT (ms) |  Speed (token/s)  |
  |-------------------|----------|----------|---------------|------------|
  | deepseek-r1:1.5b  |  35      |   9      | 242.2         |  42.77     | 
  | deepseek-llm:7b   |  22      |   2      | 761.3         |  9.58      |  
  | llama3.1:8b       |  32      |   4      | 1504.8        | 10.74      |
  | llama3.2:3b       |  39      |   5      | 426.9         | 25.32      |
  | qwen2.5:3b        |  36      |   5      | 390.0         | 26.05      |
  | qwen2.5vl:3b      |  35      |   4      | 377.6         | 26.46      |
  | qwen3:4b          |  36      |   4      | 514.6         | 17.63      |
  | phi3:3.8b         |  25      |   3      | 401.1         | 17.87      |
  | phi3.5:3.8b       |  25      |   3      | 426.5         | 16.99      |
  | phi4-mini:3.8b    |  32      |   4      | 375.5         | 20.99      |
  | gemma:7b          |  34      |   2      | 1139.6        | 8.33       |
  | gemma3n:e2b       |  35      |   5      | 352.3         | 27.51      |
  | aya:8b            |  28      |   3      | 733.4         | 9.21       |

此外，不同模型之間的適用性，除了效能指標外，也需考量模型本身的問答能力與實際應用需求。建議使用者可搭配 [LM-SYS](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)...等額外的工具，針對自身場景進行客製化評測與開發，以選出最符合需求的模型方案。
